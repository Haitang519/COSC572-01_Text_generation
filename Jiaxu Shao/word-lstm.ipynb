{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWAO0-HOEdNy",
        "colab_type": "code",
        "outputId": "5adfae3d-7375-4d8e-904d-fe546ae4913d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U3YzRuCEnUC",
        "colab_type": "code",
        "outputId": "2ae1353d-1e89-4111-d8f2-29955e879286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "file = '/content/drive/My Drive/proj/wiki_movie_plots_deduped.csv'\n",
        "\n",
        "movies_raw_df = pd.read_csv(file)\n",
        "\n",
        "movies_raw_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Release Year</th>\n",
              "      <th>Title</th>\n",
              "      <th>Origin/Ethnicity</th>\n",
              "      <th>Director</th>\n",
              "      <th>Cast</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Wiki Page</th>\n",
              "      <th>Plot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901</td>\n",
              "      <td>Kansas Saloon Smashers</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
              "      <td>A bartender is working at a saloon, serving dr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1901</td>\n",
              "      <td>Love by the Light of the Moon</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
              "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1901</td>\n",
              "      <td>The Martyred Presidents</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
              "      <td>The film, just over a minute long, is composed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1901</td>\n",
              "      <td>Terrible Teddy, the Grizzly King</td>\n",
              "      <td>American</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
              "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1902</td>\n",
              "      <td>Jack and the Beanstalk</td>\n",
              "      <td>American</td>\n",
              "      <td>George S. Fleming, Edwin S. Porter</td>\n",
              "      <td>NaN</td>\n",
              "      <td>unknown</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
              "      <td>The earliest known adaptation of the classic f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Release Year  ...                                               Plot\n",
              "0          1901  ...  A bartender is working at a saloon, serving dr...\n",
              "1          1901  ...  The moon, painted with a smiling face hangs ov...\n",
              "2          1901  ...  The film, just over a minute long, is composed...\n",
              "3          1901  ...  Lasting just 61 seconds and consisting of two ...\n",
              "4          1902  ...  The earliest known adaptation of the classic f...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qsQU_vReKN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movies_to_select = ((movies_raw_df['Genre'] == 'horror') &\n",
        "                    # Restrict to Amerian movies. \n",
        "                    (movies_raw_df['Origin/Ethnicity'] == 'American') &\n",
        "                    # Only movies from 2000.\n",
        "                    (movies_raw_df['Release Year'] > 1999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0VX5Yn7Etuq",
        "colab_type": "code",
        "outputId": "3aaead4a-de32-436e-ac0d-241e8eaae8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# plots = movies_raw_df['Plot']\n",
        "plots = movies_raw_df[movies_to_select]['Plot']\n",
        "\n",
        "print(plots.head())\n",
        "print(plots.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13617    In November 1999, tourists and fans of The Bla...\n",
            "13640    Matthew Van Helsing, the alleged descendant of...\n",
            "13681    A small group of fervent Roman Catholics belie...\n",
            "13731    Cotton Weary, now living in Los Angeles and th...\n",
            "13763    Amy Mayfield, a student at a prestigious film ...\n",
            "Name: Plot, dtype: object\n",
            "(260,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chlP3ePxJ3L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = plots.str.cat(sep=' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyGOD6B7E4U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en', disable = ['parser', 'tagger', 'ner'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztzQGlYHZ9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(doc_text):\n",
        "    # This pattern is a modification of the defaul filter from the\n",
        "    # Tokenizer() object in keras.preprocessing.text. \n",
        "    # It just indicates which patters no skip.\n",
        "    skip_pattern = '\\r\\n \\n\\n \\n\\n\\n!\"-#$%&()--.*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r '\n",
        "    \n",
        "    tokens = [token.text.lower() for token in nlp(doc_text) if token.text not in skip_pattern]\n",
        "    \n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7ieZMtkHcMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokens = set()\n",
        "# maxlen = 25+1\n",
        "# text_sequences = []\n",
        "# for i in range(len(plots)):\n",
        "#   temp = get_tokens(plots[i])\n",
        "#   for i in range(maxlen, len(temp)):\n",
        "#     seq = temp[i - maxlen: i]\n",
        "#     text_sequences.append(seq)\n",
        "#   for token in temp:\n",
        "#     tokens.add(token)\n",
        "\n",
        "# tokens = list(tokens)\n",
        "# print(tokens[0:9])\n",
        "# print(len(tokens))\n",
        "\n",
        "tokens = get_tokens(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9rwE2j_ibYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_len = 25 + 1\n",
        "\n",
        "text_sequences = []\n",
        "\n",
        "for i in range(train_len, len(tokens)):\n",
        "    # Construct sequence.\n",
        "    seq = tokens[i - train_len: i]\n",
        "    # Append.\n",
        "    text_sequences.append(seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJvf5TVEQYDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFAEcUksVbCY",
        "colab_type": "code",
        "outputId": "fa9d27a4-d5ab-46c0-9880-426bcc68fd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocabulary_size = len(tokenizer.word_counts)\n",
        "\n",
        "vocabulary_size"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqiDq-VnVzTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = np.array(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf2bnysEWUiM",
        "colab_type": "code",
        "outputId": "39cd8589-70f3-4fa2-eb86-ac8190bb1886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# select all but last word indices.\n",
        "X = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=(vocabulary_size + 1))\n",
        "print(X)\n",
        "print(X.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    8 12586 12585 ...     7   362   231]\n",
            " [12586 12585  2397 ...   362   231  2928]\n",
            " [12585  2397     2 ...   231  2928   297]\n",
            " ...\n",
            " [   20     4  1551 ...    22     1    59]\n",
            " [    4  1551  1684 ...     1    59     5]\n",
            " [ 1551  1684    22 ...    59     5     6]]\n",
            "(165845, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pazZD9XXHgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "def create_model(vocabulary_size, seq_len):\n",
        "    \n",
        "  model = Sequential()\n",
        "      \n",
        "  model.add(Embedding(input_dim=vocabulary_size, \n",
        "                      output_dim=seq_len, \n",
        "                      input_length=seq_len))\n",
        "      \n",
        "  model.add(LSTM(units=50, return_sequences=True))\n",
        "      \n",
        "  model.add(LSTM(units=50))\n",
        "      \n",
        "  model.add(Dense(units=50, activation='relu'))\n",
        "      \n",
        "  model.add(Dense(units=vocabulary_size, activation='softmax'))\n",
        "      \n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='adam', \n",
        "                metrics=['accuracy'])\n",
        "      \n",
        "  model.summary()\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSRgjRqOjMxk",
        "colab_type": "code",
        "outputId": "506883ce-6fa8-41f3-f87e-ded9121500d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "model = create_model(vocabulary_size=(vocabulary_size + 1), seq_len=X.shape[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 25, 25)            314675    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 25, 50)            15200     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12587)             641937    \n",
            "=================================================================\n",
            "Total params: 994,562\n",
            "Trainable params: 994,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAVht1KDjYtU",
        "colab_type": "code",
        "outputId": "d964424c-efca-4baa-e3e1-5588f72ebd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x=X, y=y, batch_size=128, epochs=700, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 6.9875 - accuracy: 0.0664\n",
            "Epoch 2/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 6.5578 - accuracy: 0.0817\n",
            "Epoch 3/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 6.3303 - accuracy: 0.0976\n",
            "Epoch 4/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 6.1199 - accuracy: 0.1145\n",
            "Epoch 5/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.9640 - accuracy: 0.1223\n",
            "Epoch 6/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 5.8438 - accuracy: 0.1290\n",
            "Epoch 7/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.7391 - accuracy: 0.1353\n",
            "Epoch 8/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 5.6429 - accuracy: 0.1396\n",
            "Epoch 9/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 5.5544 - accuracy: 0.1429\n",
            "Epoch 10/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.4743 - accuracy: 0.1467\n",
            "Epoch 11/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.3982 - accuracy: 0.1496\n",
            "Epoch 12/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.3255 - accuracy: 0.1524\n",
            "Epoch 13/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 5.2566 - accuracy: 0.1559\n",
            "Epoch 14/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.1911 - accuracy: 0.1595\n",
            "Epoch 15/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.1319 - accuracy: 0.1617\n",
            "Epoch 16/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 5.0736 - accuracy: 0.1645\n",
            "Epoch 17/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 5.0209 - accuracy: 0.1671\n",
            "Epoch 18/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.9693 - accuracy: 0.1695\n",
            "Epoch 19/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.9224 - accuracy: 0.1711\n",
            "Epoch 20/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.8771 - accuracy: 0.1731\n",
            "Epoch 21/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.8332 - accuracy: 0.1761\n",
            "Epoch 22/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.7922 - accuracy: 0.1784\n",
            "Epoch 23/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.7533 - accuracy: 0.1807\n",
            "Epoch 24/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.7156 - accuracy: 0.1837\n",
            "Epoch 25/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.6795 - accuracy: 0.1851\n",
            "Epoch 26/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 4.6440 - accuracy: 0.1880\n",
            "Epoch 27/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.6108 - accuracy: 0.1904\n",
            "Epoch 28/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.5796 - accuracy: 0.1925\n",
            "Epoch 29/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.5467 - accuracy: 0.1953\n",
            "Epoch 30/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.5174 - accuracy: 0.1980\n",
            "Epoch 31/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.4864 - accuracy: 0.2005\n",
            "Epoch 32/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.4583 - accuracy: 0.2027\n",
            "Epoch 33/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.4289 - accuracy: 0.2053\n",
            "Epoch 34/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.4007 - accuracy: 0.2069\n",
            "Epoch 35/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.3751 - accuracy: 0.2100\n",
            "Epoch 36/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.3491 - accuracy: 0.2122\n",
            "Epoch 37/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.3232 - accuracy: 0.2142\n",
            "Epoch 38/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.2967 - accuracy: 0.2173\n",
            "Epoch 39/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 4.2743 - accuracy: 0.2188\n",
            "Epoch 40/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.2491 - accuracy: 0.2220\n",
            "Epoch 41/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.2261 - accuracy: 0.2231\n",
            "Epoch 42/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.2039 - accuracy: 0.2265\n",
            "Epoch 43/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.1816 - accuracy: 0.2282\n",
            "Epoch 44/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.1580 - accuracy: 0.2312\n",
            "Epoch 45/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.1358 - accuracy: 0.2332\n",
            "Epoch 46/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 4.1190 - accuracy: 0.2347\n",
            "Epoch 47/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.0971 - accuracy: 0.2365\n",
            "Epoch 48/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.0751 - accuracy: 0.2393\n",
            "Epoch 49/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.0546 - accuracy: 0.2416\n",
            "Epoch 50/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.0352 - accuracy: 0.2436\n",
            "Epoch 51/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 4.0174 - accuracy: 0.2450\n",
            "Epoch 52/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.9976 - accuracy: 0.2468\n",
            "Epoch 53/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.9793 - accuracy: 0.2492\n",
            "Epoch 54/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.9630 - accuracy: 0.2515\n",
            "Epoch 55/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.9423 - accuracy: 0.2537\n",
            "Epoch 56/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.9223 - accuracy: 0.2559\n",
            "Epoch 57/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.9062 - accuracy: 0.2580\n",
            "Epoch 58/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.8866 - accuracy: 0.2600\n",
            "Epoch 59/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.8684 - accuracy: 0.2628\n",
            "Epoch 60/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.8539 - accuracy: 0.2631\n",
            "Epoch 61/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.8345 - accuracy: 0.2644\n",
            "Epoch 62/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.8189 - accuracy: 0.2669\n",
            "Epoch 63/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.8010 - accuracy: 0.2691\n",
            "Epoch 64/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.7853 - accuracy: 0.2707\n",
            "Epoch 65/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.7706 - accuracy: 0.2726\n",
            "Epoch 66/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.7541 - accuracy: 0.2746\n",
            "Epoch 67/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.7368 - accuracy: 0.2769\n",
            "Epoch 68/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.7214 - accuracy: 0.2777\n",
            "Epoch 69/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.7069 - accuracy: 0.2798\n",
            "Epoch 70/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.6901 - accuracy: 0.2823\n",
            "Epoch 71/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.6785 - accuracy: 0.2831\n",
            "Epoch 72/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.6631 - accuracy: 0.2850\n",
            "Epoch 73/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.6497 - accuracy: 0.2875\n",
            "Epoch 74/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.6350 - accuracy: 0.2886\n",
            "Epoch 75/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.6231 - accuracy: 0.2903\n",
            "Epoch 76/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.6084 - accuracy: 0.2912\n",
            "Epoch 77/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.5943 - accuracy: 0.2933\n",
            "Epoch 78/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.5808 - accuracy: 0.2955\n",
            "Epoch 79/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.5690 - accuracy: 0.2966\n",
            "Epoch 80/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.5573 - accuracy: 0.2983\n",
            "Epoch 81/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.5451 - accuracy: 0.3002\n",
            "Epoch 82/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.5311 - accuracy: 0.3017\n",
            "Epoch 83/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.5233 - accuracy: 0.3025\n",
            "Epoch 84/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.5150 - accuracy: 0.3040\n",
            "Epoch 85/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4945 - accuracy: 0.3061\n",
            "Epoch 86/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4865 - accuracy: 0.3084\n",
            "Epoch 87/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4766 - accuracy: 0.3095\n",
            "Epoch 88/700\n",
            "1296/1296 [==============================] - 23s 18ms/step - loss: 3.4661 - accuracy: 0.3105\n",
            "Epoch 89/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4553 - accuracy: 0.3116\n",
            "Epoch 90/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.4446 - accuracy: 0.3129\n",
            "Epoch 91/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.4349 - accuracy: 0.3145\n",
            "Epoch 92/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4224 - accuracy: 0.3160\n",
            "Epoch 93/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4155 - accuracy: 0.3165\n",
            "Epoch 94/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.4052 - accuracy: 0.3185\n",
            "Epoch 95/700\n",
            "1296/1296 [==============================] - 24s 18ms/step - loss: 3.3949 - accuracy: 0.3196\n",
            "Epoch 96/700\n",
            "1296/1296 [==============================] - 27s 21ms/step - loss: 3.3859 - accuracy: 0.3211\n",
            "Epoch 97/700\n",
            "1296/1296 [==============================] - 27s 21ms/step - loss: 3.3768 - accuracy: 0.3227\n",
            "Epoch 98/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3685 - accuracy: 0.3233\n",
            "Epoch 99/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3583 - accuracy: 0.3240\n",
            "Epoch 100/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.3498 - accuracy: 0.3259\n",
            "Epoch 101/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3459 - accuracy: 0.3262\n",
            "Epoch 102/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3324 - accuracy: 0.3268\n",
            "Epoch 103/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3263 - accuracy: 0.3294\n",
            "Epoch 104/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3133 - accuracy: 0.3312\n",
            "Epoch 105/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3063 - accuracy: 0.3309\n",
            "Epoch 106/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.3001 - accuracy: 0.3320\n",
            "Epoch 107/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2896 - accuracy: 0.3344\n",
            "Epoch 108/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2841 - accuracy: 0.3351\n",
            "Epoch 109/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2787 - accuracy: 0.3358\n",
            "Epoch 110/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2668 - accuracy: 0.3376\n",
            "Epoch 111/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2653 - accuracy: 0.3373\n",
            "Epoch 112/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2541 - accuracy: 0.3390\n",
            "Epoch 113/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2367 - accuracy: 0.3413\n",
            "Epoch 114/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2325 - accuracy: 0.3413\n",
            "Epoch 115/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2281 - accuracy: 0.3418\n",
            "Epoch 116/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2192 - accuracy: 0.3435\n",
            "Epoch 117/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2110 - accuracy: 0.3442\n",
            "Epoch 118/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.2068 - accuracy: 0.3453\n",
            "Epoch 119/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1916 - accuracy: 0.3471\n",
            "Epoch 120/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1879 - accuracy: 0.3478\n",
            "Epoch 121/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1813 - accuracy: 0.3485\n",
            "Epoch 122/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1741 - accuracy: 0.3501\n",
            "Epoch 123/700\n",
            "1296/1296 [==============================] - 24s 19ms/step - loss: 3.1657 - accuracy: 0.3505\n",
            "Epoch 124/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1588 - accuracy: 0.3517\n",
            "Epoch 125/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1508 - accuracy: 0.3533\n",
            "Epoch 126/700\n",
            "1296/1296 [==============================] - 25s 20ms/step - loss: 3.1434 - accuracy: 0.3538\n",
            "Epoch 127/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1361 - accuracy: 0.3555\n",
            "Epoch 128/700\n",
            "1296/1296 [==============================] - 25s 20ms/step - loss: 3.1311 - accuracy: 0.3560\n",
            "Epoch 129/700\n",
            "1296/1296 [==============================] - 25s 20ms/step - loss: 3.1204 - accuracy: 0.3572\n",
            "Epoch 130/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1161 - accuracy: 0.3584\n",
            "Epoch 131/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1095 - accuracy: 0.3586\n",
            "Epoch 132/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.1032 - accuracy: 0.3598\n",
            "Epoch 133/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.0972 - accuracy: 0.3598\n",
            "Epoch 134/700\n",
            "1296/1296 [==============================] - 25s 19ms/step - loss: 3.0936 - accuracy: 0.3614\n",
            "Epoch 135/700\n",
            " 651/1296 [==============>...............] - ETA: 12s - loss: 3.0324 - accuracy: 0.3681Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZjQKqdTXEgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import dump\n",
        "\n",
        "dump(tokenizer, open('tokenizer', 'wb'))\n",
        "\n",
        "model.save('/content/drive/My Drive/proj/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8STga1Va_LA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text2(model, tokenizer, seq_len, seed_text, num_gen_words, temperature):\n",
        "    \n",
        "    output_text = []\n",
        "    \n",
        "    input_text = seed_text\n",
        "    \n",
        "    for i in range(num_gen_words):\n",
        "        # Encode input text. \n",
        "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "         # Add if the input tesxt does not have length len_0.\n",
        "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "        # Get learned distribution.\n",
        "        pred_distribution = model.predict(pad_encoded, verbose=0)[0]\n",
        "        \n",
        "        # Apply temperature transformation.\n",
        "        new_pred_distribution = np.power(pred_distribution, (1 / temperature)) \n",
        "        new_pred_distribution = new_pred_distribution / new_pred_distribution.sum()\n",
        "        \n",
        "        # Sample from modified distribution.\n",
        "        choices = range(new_pred_distribution.size)\n",
        " \n",
        "        pred_word_ind = np.random.choice(a=choices, p=new_pred_distribution)\n",
        "        \n",
        "        # Convert from numeric to word. \n",
        "        pred_word = tokenizer.index_word[pred_word_ind]\n",
        "        # Attach predicted word. \n",
        "        input_text += ' ' + pred_word\n",
        "        # Append new word to the list. \n",
        "        output_text.append(pred_word)\n",
        "        \n",
        "    return ' '.join(output_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8epSv3qcmRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = len(plots)\n",
        "for i in range(5):\n",
        "  seed_text = plots.iloc[np.random.randint(l)][:150]\n",
        "  print(seed_text)\n",
        "  generated_text = generate_text2(model=model, \n",
        "                  tokenizer=tokenizer,\n",
        "                  seq_len=X.shape[1], \n",
        "                  seed_text=seed_text, \n",
        "                  num_gen_words=60, \n",
        "                  temperature=0.9)\n",
        "  print(seed_text + ' ' + generated_text + ' ...')\n",
        "  generated_text = generate_text2(model=model, \n",
        "                  tokenizer=tokenizer,\n",
        "                  seq_len=X.shape[1], \n",
        "                  seed_text=seed_text, \n",
        "                  num_gen_words=60, \n",
        "                  temperature=0.5)\n",
        "  print(seed_text + ' ' + generated_text + ' ...')\n",
        "  generated_text = generate_text2(model=model, \n",
        "                  tokenizer=tokenizer,\n",
        "                  seq_len=X.shape[1], \n",
        "                  seed_text=seed_text, \n",
        "                  num_gen_words=60, \n",
        "                  temperature=0.1)\n",
        "  print(seed_text + ' ' + generated_text + ' ...')\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}