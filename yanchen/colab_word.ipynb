{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY1S1cTbcB0A",
        "colab_type": "code",
        "outputId": "69945a83-a448-42a2-d7f9-5fae53be5000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import argparse\n",
        "import os, random\n",
        "from collections import Counter\n",
        "\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Dropout\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.initializers import Constant\n",
        "import pandas as pd \n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "\n",
        "UNK = '[UNK]'\n",
        "PAD = '[PAD]'\n",
        "START = '<s>'\n",
        "END = '</s>'\n",
        "vocab = Counter()\n",
        "data = []\n",
        "\n",
        "contractions = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there had\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\"\n",
        "}\n",
        "\n",
        "def clean_plot(plot, remove_stopwords=False, lemma=True):\n",
        "    # lowercase\n",
        "    plot = plot.lower()\n",
        "    # remove cite notation eg.[1]\n",
        "    plot = re.sub('\\[\\d*\\]', '', plot)\n",
        "    plot = re.sub('\\ \\ *', ' ', plot)\n",
        "    # remove non-letter characters\n",
        "    plot = re.sub(r\"[^a-zA-Z]+\", r\" \", plot)\n",
        "\n",
        "    # expand contraction\n",
        "    wnl = WordNetLemmatizer()\n",
        "    plot = plot.strip().split(' ')\n",
        "    new_plot = []\n",
        "    for word in plot:\n",
        "        if word in contractions:\n",
        "            new_plot.append(contractions[word])\n",
        "        else:\n",
        "            if lemma:\n",
        "                word = wnl.lemmatize(word)\n",
        "            new_plot.append(word)\n",
        "    plot = \" \".join(new_plot)\n",
        "\n",
        "    # remove stopwords\n",
        "    if remove_stopwords:\n",
        "        plot = plot.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        plot = [w for w in plot if not w in stops]\n",
        "        plot = \" \".join(plot)\n",
        "    return plot\n",
        "\n",
        "def get_vocabulary_and_data(plots, max_vocab_size=20000):\n",
        "    vocab = Counter()\n",
        "    data = []\n",
        "    lens = []\n",
        "    count = 0\n",
        "    for line in plots.Plot:\n",
        "        if count%1000==0:\n",
        "            print(count)\n",
        "        count+=1\n",
        "        line = clean_plot(line)\n",
        "        sent = [START]\n",
        "        vocab[START]+=1\n",
        "        vocab[END]+=1\n",
        "        lens.append(len(line.strip().split(' ')))\n",
        "        for tok in line.strip().split(' '):\n",
        "            sent.append(tok)\n",
        "            vocab[tok]+=1\n",
        "        sent.append(END)\n",
        "        data.append(sent)\n",
        "    # if max_vocab_size:\n",
        "    vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1],reverse=True)}\n",
        "    vocab = list(vocab)\n",
        "    vocab = vocab[:max_vocab_size]\n",
        "    vocab = [UNK, PAD] + vocab\n",
        "    n = len(lens)\n",
        "    Q1 = lens[int((n+1)/4)]\n",
        "    Q3 = lens[int(3*(n + 1)/4)]\n",
        "    MAX_SEQUENCE_LENGTH = int(Q3 + 0.5 * (Q3 - Q1))\n",
        "\n",
        "    return {k:v for v,k in enumerate(vocab)}, data,MAX_SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "\n",
        "def vectorize_sequence(seq, vocab):\n",
        "    seq = [tok if tok in vocab else UNK for tok in seq]\n",
        "    return [vocab[tok] for tok in seq]\n",
        "\n",
        "\n",
        "def unvectorize_sequence(seq, vocab):\n",
        "    translate = sorted(vocab.keys(),key=lambda k:vocab[k])\n",
        "    return [translate[i] for i in seq]\n",
        "\n",
        "\n",
        "def one_hot_encode_label(label, vocab):\n",
        "    vec = [1.0 if l==label else 0.0 for l in vocab]\n",
        "    return vec\n",
        "\n",
        "\n",
        "def batch_generator_lm(data, vocab, batch_size=1):\n",
        "    while True:\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        for sent in data:\n",
        "            batch_x.append(vectorize_sequence(sent, vocab))\n",
        "            batch_y.append([one_hot_encode_label(token, vocab) for token in shift_by_one(sent)])\n",
        "            if len(batch_x) >= batch_size:\n",
        "                # Pad Sequences in batch to same length\n",
        "                batch_x = pad_sequences(batch_x, vocab[PAD])\n",
        "                batch_y = pad_sequences(batch_y, one_hot_encode_label(PAD, vocab))\n",
        "                yield np.array(batch_x), np.array(batch_y)\n",
        "                batch_x = []\n",
        "                batch_y = []\n",
        "\n",
        "\n",
        "def describe_data(data, generator):\n",
        "    batch_x, batch_y = [], []\n",
        "    for bx, by in generator:\n",
        "        batch_x = bx\n",
        "        batch_y = by\n",
        "        break\n",
        "    print('Data example:',data[0])\n",
        "    print('Batch input shape:', batch_x.shape)\n",
        "    print('Batch output shape:', batch_y.shape)\n",
        "\n",
        "\n",
        "def pad_sequences(batch_x, pad_value):\n",
        "    ''' This function should take a batch of sequences of different lengths\n",
        "        and pad them with the pad_value token so that they are all the same length.\n",
        "\n",
        "        Assume that batch_x is a list of lists.\n",
        "    '''\n",
        "    pad_length = len(max(batch_x, key=lambda x: len(x)))\n",
        "    for i, x in enumerate(batch_x):\n",
        "        if len(x) < pad_length:\n",
        "            batch_x[i] = x + ([pad_value] * (pad_length - len(x)))\n",
        "\n",
        "    return batch_x\n",
        "\n",
        "\n",
        "def generate_text(language_model, vocab):\n",
        "    prediction = [START]\n",
        "    while not (prediction[-1] == END or len(prediction)>=100):\n",
        "        next_token_one_hot = language_model.predict(np.array([[vocab[p] for p in prediction]]), batch_size=1)[0][-1]\n",
        "        threshold = random.random()\n",
        "        sum = 0\n",
        "        next_token = 0\n",
        "        for i,p in enumerate(next_token_one_hot):\n",
        "            sum += p\n",
        "            if sum>threshold:\n",
        "                next_token = i\n",
        "                break\n",
        "        for w, i in vocab.items():\n",
        "            if i==next_token:\n",
        "                prediction.append(w)\n",
        "                break\n",
        "    return prediction\n",
        "\n",
        "# TODO\n",
        "def load_pretrained_embeddings(glove_file, vocab):\n",
        "    embedding_matrix = np.zeros((len(vocab), 100))\n",
        "    with open(glove_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "            # Each line will be a word and a list of floats, separated by spaces.\n",
        "            # If the word is in your vocabulary, create a numpy array from the list of floats.\n",
        "            # Assign the array to the correct row of embedding_matrix.\n",
        "            if word in vocab:\n",
        "                embedding_matrix[vocab[word]] = coefs\n",
        "    embedding_matrix[vocab[UNK]] = np.random.randn(100)\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def shift_by_one(seq):\n",
        "    '''\n",
        "    input: ['<s>', 'The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>']\n",
        "    output: ['The', 'dog', 'chased', 'the', 'cat', 'around', 'the', 'house', '</s>', '[PAD]']\n",
        "    '''\n",
        "    result = seq[1:]\n",
        "    result.append('[PAD]')\n",
        "    return result\n",
        "    \n",
        "\n",
        "def clean_data(data,vocab,MAX_SEQUENCE_LENGTH):\n",
        "    data1 = []\n",
        "    count = 0\n",
        "    for line in data:\n",
        "        if count%1000==0:\n",
        "            print(count)\n",
        "        count+=1\n",
        "        if len(line)>MAX_SEQUENCE_LENGTH:\n",
        "            line1 = line[:MAX_SEQUENCE_LENGTH]\n",
        "            line1.append(END)\n",
        "            data1.append(line1)\n",
        "        else: \n",
        "            data1.append(line)\n",
        "    \n",
        "    train_data = []\n",
        "    count = 0\n",
        "    for sent in data1:\n",
        "        count +=1\n",
        "        [w if w in vocab else 'UNK' for w in sent]\n",
        "        train_data.append([w if w in vocab else 'UNK' for w in sent])\n",
        "        if count%1000==0:\n",
        "            print(count)\n",
        "    return train_data\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    for i in range(10):\n",
        "        prediction = [START]\n",
        "        while not (prediction[-1] == END or len(prediction)>=100):\n",
        "            next_token_one_hot = language_model.predict(np.array([[vocab[p] for p in prediction]]), batch_size=1)[0][-1]\n",
        "            threshold = random.random()\n",
        "            sum = 0\n",
        "            next_token = 0\n",
        "            for i,p in enumerate(next_token_one_hot):\n",
        "                sum += p\n",
        "                if sum>threshold:\n",
        "                    next_token = i\n",
        "                    break\n",
        "            for w, i in vocab.items():\n",
        "                if i==next_token:\n",
        "                    prediction.append(w)\n",
        "                    break\n",
        "        sys.stdout.write(prediction)\n",
        "        sys.stdout.flush()\n",
        "drive.mount('/content/gdrive/')\n",
        "glove_file = '/content/gdrive/My Drive/Colab Notebooks/glove.6B.100d.txt'\n",
        "data_path = '/content/gdrive/My Drive/Colab Notebooks/wiki_movie_plots_deduped.csv'\n",
        "plots = pd.read_csv(data_path, sep=',', encoding='latin1')\n",
        "train_plots, test_plots = train_test_split(plots,test_size = 0.15)\n",
        "dev_plots, test_plots = train_test_split(test_plots,test_size = 0.5)\n",
        "\n",
        "vocab, train_data, MAX_SEQUENCE_LENGTH = get_vocabulary_and_data(train_plots)\n",
        "_, dev_data,_ = get_vocabulary_and_data(dev_plots)\n",
        "_, test_data,_ = get_vocabulary_and_data(test_plots)\n",
        "\n",
        "train_data = clean_data(train_data,vocab,MAX_SEQUENCE_LENGTH)\n",
        "dev_data = clean_data(dev_data,vocab,MAX_SEQUENCE_LENGTH)\n",
        "test_data = clean_data(test_data,vocab,MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "embedding_matrix = load_pretrained_embeddings(glove_file, vocab)\n",
        "describe_data(train_data, batch_generator_lm(train_data, vocab, 10))\n",
        "weights = ModelCheckpoint(filepath = 'model10.h5')\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "\n",
        "language_model = Sequential()\n",
        "language_model.add(Embedding(len(vocab), 100,embeddings_initializer=Constant(embedding_matrix),trainable=False))\n",
        "language_model.add(Dropout(0.2))\n",
        "language_model.add(LSTM(100, return_sequences=True))\n",
        "language_model.add(Dropout(0.2))\n",
        "language_model.add(TimeDistributed(Dense(len(vocab), activation='softmax')))\n",
        "\n",
        "language_model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists('model10.h5'):\n",
        "    language_model.fit_generator(batch_generator_lm(train_data, vocab,10),\n",
        "                             epochs=60, steps_per_epoch=len(train_data)/10,callbacks=[print_callback,weights])\n",
        "else:\n",
        "    language_model = load_model('model10.h5')\n",
        "    print(language_model.summary())\n",
        "    language_model.fit_generator(batch_generator_lm(train_data, vocab,10),\n",
        "                             epochs=60, steps_per_epoch=len(train_data)/10,callbacks=[print_callback,weights])\n",
        "\n",
        "# Evaluation\n",
        "loss, acc = language_model.evaluate_generator(batch_generator_lm(dev_data, vocab),\n",
        "                                              steps=len(dev_data))\n",
        "print('Dev Loss:', loss, 'Dev Acc:', acc)\n",
        "loss, acc = language_model.evaluate_generator(batch_generator_lm(test_data, vocab),\n",
        "                                              steps=len(test_data))\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "1000\n",
            "2000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "1000\n",
            "2000\n",
            "Data example: ['<s>', 'abhiram', 'raghu', 'is', 'a', 'rich', 'spoilt', 'brat', 'who', 'belief', 'only', 'in', 'class', 'and', 'hate', 'the', 'company', 'of', 'poor', 'people', 'with', 'low', 'profile', 'he', 'is', 'in', 'deep', 'love', 'with', 'dr', 'janaki', 'an', 'orphan', 'who', 'aim', 'to', 'serve', 'poor', 'people', 'a', 'a', 'doctor', 'and', 'give', 'them', 'proper', 'medication', 'though', 'he', 'is', 'in', 'love', 'with', 'her', 'abhiram', 'doe', 'not', 'like', 'the', 'path', 'UNK', 'by', 'janaki', 'in', 'helping', 'poor', 'and', 'needy', 'this', 'break', 'their', 'relationship', 'apart', 'and', 'janaki', 'walk', 'away', 'to', 'remote', 'place', 'on', 'her', 'job', 'not', 'able', 'to', 'withstand', 'the', 'agony', 'of', 'separation', 'abhiram', 'set', 'on', 'a', 'journey', 'in', 'finding', 'janaki', 'again', 'to', 'carry', 'on', 'the', 'relationship', 'meanwhile', 'he', 'meet', 'UNK', 'seenu', 'kitty', 'a', 'petty', 'thief', 'who', 'is', 'an', 'expert', 'in', 'robbing', 'the', 'two', 'wheeler', 'vehicle', 'he', 'strike', 'a', 'deal', 'with', 'seenu', 'and', 'further', 'embarks', 'his', 'journey', 'in', 'search', 'of', 'janaki', 'en', 'route', 'his', 'search', 'abhiram', 'begin', 'to', 'slowly', 'realize', 'the', 'real', 'world', 'and', 'the', 'noble', 'intention', 'janaki', 'had', 'in', 'her', 'service', 'finally', 'the', 'lover', 'meet', 'up', 'and', 'what', 'happens', 'next', 'form', 'the', 'rest', 'of', 'the', 'story', '</s>']\n",
            "Batch input shape: (10, 517)\n",
            "Batch output shape: (10, 517, 20002)\n",
            "Epoch 1/60\n",
            " 668/2965 [=====>........................] - ETA: 6:28:54 - loss: 4.1826 - accuracy: 0.4480"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}